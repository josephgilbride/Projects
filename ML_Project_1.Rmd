---
title: "APANPS5205 Homework 1"
output: pdf_document
---


For this assignment, the following libraries are required, please install and load them by running the following:

```{r, message=FALSE, warning=FALSE}

required_packages = c( "tidyverse", "corrplot", "gridExtra",
                       "GGally", "cluster", "factoextra", 
                       "mlbench", "class", "ggplot2", "FNN",
                       "caret", "plot3D", "Rtsne")

packages_to_install = setdiff(required_packages, installed.packages()[,"Package"])

if (length(packages_to_install)!=0) {
  install.packages(packages_to_install)
}

library(mlbench)
library(ggplot2)
library(class)
library(FNN)
library(caret)
library(dplyr)
library(tidyr)
library(corrplot)
library(cluster)
library(plot3D)
library(Rtsne)
library(GGally)
library(factoextra)

# let's set the seed to 0 for consistency
set.seed(0)

```


## Question 1: Data Visualization (10 pts)

For question 1, we'll be working with the wine dataset from UCI Machine Learning Repository. The data can be downloaded directly from the repository (see code below).

### 1.1 Read in the data and report its dimension. (2 pts)

```{r cars}
# read in the data using the following
wine = read.csv(
  url("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"),
  header = F, 
  col.names = c("Wine", "Alcohol", "Malic.acid", "Ash", 
                "Acl", "Mg", "Phenols", "Flavanoid",
                "Nonflavanoid.phenols", "Proanth", "Color.int", 
                "Hue", "OD", "Proline"))

# display the first few columns using head()

head(wine)

# report dimension

dim(wine)

```

### 1.2 Show the summary statistics of the dataset. (2 pts)

```{r}
# show summary statistics of the data

summary(wine)

```

### 1.3 What is the pairwise relationship of the first five variables? Use the pairs function to display scatterplots of each pair of these variables. (2 pts)

```{r}
# pair plot of wine

pairs(wine)

ggpairs(wine)


```

### 1.4 Plot a histogram of each attribute. (2 pts)

```{r}

# histogram plot of each attribute

ggplot(gather(wine), aes(value)) + 
        geom_histogram(bins = 10) + 
       facet_wrap(~key, scales = 'free_x')

```

### 1.5 To understand the relationship between each pair of attributes, construct a correlation matrix using corrplot(). (2 pts)

```{r}
# correlation plot

corrplot(cor(wine))

```


## Question 2. Clustering (10 pts)

### 2.1 Read in the iris data set. Subset your data to only include "Sepal Length", "Sepal Width", "Petal Length", and "Petal Width". Rescale your data such that the values are between 0 and 1. Appply K-means clustering with the number of clusters as 3. Compare your clusters with the actual class using table(). (3 pts)

```{r}

# read in "iris" using data()

data(iris)

# split your data in to a features ("Sepal Length", "Sepal Width", "Petal Length",
# and "Petal Width") and target ("Species)

iris2 = subset(iris,select=-c(Species))

# normalize your features

iris3 = scale(iris2)

# fit a k-means model with k = 3

km = kmeans(iris3,centers = 3,nstart=25)
fviz_cluster(km, data = iris2)

# compare your cluster results with the target variable using table()

table(iris$Species)


```


### 2.2 Draw a 2D clustering plot using clusplot(). (2 pts)

```{r}
# plot a 2D clustering plot using clustplot()

clusters <- pam(iris2, 3)$clustering
clusplot(iris2, clusters)

```

### 2.3 Draw a 3D plot using scatter3D. (2 pts)

```{r}
# draw a 3D plot using scatter3D

scatter3D(iris2$Sepal.Length, iris2$Sepal.Width, iris2$Petal.Length, iris2$Petal.Width)


```

### 2.4 Visualize high-dimensional (>3 dimensions) data by first reducing the number of dimensions to 2 using t-SNE. Plot your t-SNE dimensions colored by the class labels. (3 pts)

```{r}
# transform your features data into a matrix

mat = as.matrix(iris2)

# use Rtsne() to reduce your dimensions, set perplexity = 20, theta = 0.5, dims = 2

tsne = Rtsne(mat, perplexity = 20, theta = 0.5, dims = 2, check_duplicates = FALSE)

# display the results of t-SNE colored by the cluster labels

dtsne = as.data.frame(tsne$Y)
fviz_cluster(km, dtsne)
```


## Question 3. kNN Classification (10 pts)


### 3.1 Read in "PimaIndiansDiabetes" dataset using "data(PimaIndiansDiabetes)". Describe the features in the data using summary(). Plot a histogram of the variable "age" and describe its distribution. Compute the proportion of individuals with diabetes. (2 pt)

```{r}

# read in the data, store the data as "df"

data(PimaIndiansDiabetes)
df = PimaIndiansDiabetes

# describe the data using summary()

summary(df)

# plot histogram of age

hist(df$age)

# proportion of individuals with diabetes

count(subset(df, diabetes=='pos'))/nrow(df)*100

```

The distribution of "age" variable is skewed with more younger individuals than older.

### 3.2 Perform a scatter plot of glucose and mass, colored by diabetes. (2 pt)

```{r}
# convert "pos" in diabetes variable to factor

df$diabetes = as.factor(df$diabetes)

# plot glucose vs mass, colored by diabetes

ggplot(df, aes(glucose, mass, colour = diabetes)) +
  geom_point()

```

### 3.3 Use "knn3" command to build a k-NN classifier, setting k to 2, 50, and 200. Visualize the decision boundaries using the "decision_boundary_plot" function below. (3 pts)

```{r}

# function to plot decision boundary
decision_boundary_plot <- function(model, data, x1_var, x2_var, y_var, resolution = 100, title) {
  
  x1 = data[, x1_var]
  x2 = data[, x2_var]
  y = data[, y_var]
  
  # make grid
  xs1 <- seq(min(x1), max(x1), length.out = resolution)
  xs2 <- seq(min(x2), max(x2), length.out = resolution)
  g <- cbind(rep(xs1, each=resolution), rep(xs2, time = resolution))
  g <- as.data.frame(g)
  names(g) = c('glucose', 'mass')
  
  p <- predict(model, g, type = "class")
  
    plt = ggplot() + 
    geom_point(aes(g[, 1], g[, 2], col = p), size = 0.1) +
    geom_point(aes(x1, x2, col = y), size = 2) + 
    geom_contour(
      aes(x = g[, 1], y = g[, 2],z = as.integer(p)), 
      col = 'black', size = 0.1) +
    xlab('glucose') + ylab('mass') + scale_colour_discrete(name='diabetes') +
    ggtitle(title)
  
  return(plt)
}
```

```{r}
# build a k-NN classifier, setting k to 2

knnmod = knn3(formula=diabetes~mass+glucose, data=df, k=2)

# visualize the decision boundary of k-NN, k = 2

decision_boundary_plot(knnmod, df, 2, 6, 9, resolution = 100, title= "K=2")

# build a k-NN classifier, setting k to 50

knnmod2 = knn3(formula=diabetes~mass+glucose, data=df, k=50)

# visualize the decision boundary of k-NN, k = 50

decision_boundary_plot(knnmod2, df, 2, 6, 9, resolution = 100, title= "K=50")

# build a k-NN classifier, setting k to 200

knnmod3 = knn3(formula=diabetes~mass+glucose, data=df, k=200)

# visualize the decision boundary of k-NN, k = 200

decision_boundary_plot(knnmod3, df, 2, 6, 9, resolution = 100, title= "K=200")

```

### 3.4 What happens to the decision boundary for k-NN as k increases? Intuitively, why does this happen? From the visualizations, which k is optimal? (3 pts)

# As k increases, the boundary becomes less defined, since there are more neighbors considered in the knn calculation.  From the visualizations, it appears that as k between 2 <> 50 is optimal.


## Question 4. Regression (10 pts)

### 4.1 Read in "airquality" dataset using "data(airquality)". Describe the data using summary(). Use pair() to observe the pairwise relationships between the variables. Do most of these pairwise relationships seem linear or nonlinear? (2 pts)

```{r}

# read in "airquality"

data(airquality)

# store the data as df

dfair = airquality

# describe the data using summary()

summary(dfair)

# observe the pairwise relationships between the variables 

pairs(dfair)
ggpairs(dfair)

```

From the pair plot, most of the relationships appear to be nonlinear.


### 4.2 Regress Temp (y) on Ozone (x) using linear regression and subsequently a third-order polynomial regression. Perform a scatter plot of "Ozone" and "Temp" with the fitted linear and polynomial regressions. Which is better? (2 pts) 

```{r}

# read in airquality


# subset the data to include only "Ozone" and "Temp". Remove missing values.

dfair2 = cbind(dfair[,1], dfair[,4])
dfair2 = na.omit(dfair2)
dfair2 = data.frame(dfair2)
names(dfair2) = c('Ozone', 'Temp')
# regress Temp (y) on Ozone (x) using linear regression

airmod = lm(Temp~Ozone, dfair2)

ggplot(dfair2, aes(x = Ozone, y = Temp)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

# regression Temp (y) on Ozone (x) using 3rd-degree polynomial regression

airmod2 = lm(Temp~poly(Ozone), dfair2)

ggplot(dfair2, aes(x = Ozone, y = Temp)) + 
  geom_point() +
  stat_smooth(method = "loess", col = "red")

# plot Temp vs Ozone with the fitted regression line

ggplot(dfair2, aes(x = Ozone, y = Temp)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  stat_smooth(method = "loess", col = "orange")

```

Visually, polynomial regression fits the data better.




### 4.3 Use "knn.reg" from "FNN" library to perform a k-NN regression to predict "Temp" given "Ozone". Set k to 2, 10, and 100. Perform a scatter plot of Ozone and Temp with the kNN predictions. (3 pts)

```{r}
# fit a kNN regression with k = 2

knnregmod = knn.reg(dfair2, y=dfair2$Temp, k=2)
dfplot = cbind(dfair2, data.frame(knnregmod$pred))
dfplot

# plot Temp vs Ozone with kNN predictions, k = 2

ggplot(dfair2, aes(x = Ozone, y = Temp))+
  geom_point()+
  geom_point(aes(dfplot$knnregmod.pred, col = "orange"))+
  scale_colour_discrete(name='pred') +
  stat_smooth(method = "loess", col = "red")+
   stat_smooth(method = "lm", col = "yellow")
# fit a kNN regression with k = 10

knnregmod2 = knn.reg(dfair2, y=dfair2$Temp, k=10)
dfplot2 = cbind(dfair2, data.frame(knnregmod2$pred))
dfplot2

# plot Temp vs Ozone with kNN predictions, k = 10

ggplot(dfair2, aes(x = Ozone, y = Temp))+
  geom_point()+
  geom_point(aes(dfplot2$knnregmod2.pred, col = 'orange'))+
  scale_colour_discrete(name='pred') +
  stat_smooth(method = "loess", col = "red")+
  stat_smooth(method = "lm", col = "yellow")


# fit a kNN regression with k = 100

knnregmod3 = knn.reg(dfair2, y=dfair2$Temp, k=100)
dfplot3 = cbind(dfair2, data.frame(knnregmod3$pred))
dfplot3


# plot Temp vs Ozone with kNN predictions, k = 100

ggplot(dfair2, aes(x = Ozone, y = Temp))+
  geom_point()+
  geom_point(aes(dfplot3$knnregmod3.pred, col = 'orange'))+
  scale_colour_discrete(name='pred') +
  stat_smooth(method = "loess", col = "red") +
  stat_smooth(method = "lm", col = "yellow")

```


### 4.4 Describe the predictions as k increases. Intuitively, why does this happen? Is this behavior similar in a k-NN classification? Which k is optimal? Compare kNN regression with linear regression. (3 pts)


As k increases, or number of neighbors, predictions become less staggered. (The predictions observed actually align with the bend in the polynomial regression with higher k values).  In this situation, the lower the k value, the better (k<2).  Nearest neighbors in this situation has a max value of 116, or nrows.

kNN Predictions will be more accurate with more data points.  Analysis of kNN regression vs. linear regression would be more appropriate then.

